{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Input/output\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "ESRF data come in (too many) different formats:\n",
    "\n",
    "* Specfile\n",
    "* EDF\n",
    "* HDF5\n",
    "\n",
    "and specific detector formats:\n",
    "\n",
    "* MarCCD\n",
    "* Pilatus CBF\n",
    "* Dectris Eiger\n",
    "* …\n",
    "\n",
    "\n",
    "HDF5 is expected to become the standard ESRF data format. Some beamlines have already switched."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Accessing ESRF data\n",
    "\n",
    "### Libraries\n",
    "\n",
    "\n",
    "* h5py\n",
    "    * Access to HDF5 files\n",
    "* FabIO\n",
    "    * Provides access to several image data formats\n",
    "    * Developed as part of the Fable project, initially an ID11 development.\n",
    "    * Managed by the DAU\n",
    "* silx\n",
    "    * Started in 2015\n",
    "    * Will provide input/output for PyMCA\n",
    "    * Also provides fitting, image processing, plotting, a set of widgets…\n",
    "    * Managed by the DAU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "Those are already available for most ESRF computers\n",
    "\n",
    "```bash\n",
    ">>> apt-get install python3-silx python3-fabio python3-h5py\n",
    "```\n",
    "\n",
    "Cross platform (Available for Windows, Linux, Mac OS X)\n",
    "```bash\n",
    ">>> pip install silx fabio h5py\n",
    "```\n",
    "\n",
    "\n",
    "Also available from source code (under MIT license)\n",
    "\n",
    "* https://github.com/silx-kit/silx\n",
    "* https://github.com/silx-kit/fabio\n",
    "* https://github.com/h5py/h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Spec files\n",
    "\n",
    "* text format from Spec\n",
    "* contains evolution of measurments and instruments during a scan\n",
    "* we do not recommand to use this format anymore\n",
    "* silx provides a HDF5-like read access to Spec files\n",
    "\n",
    "### Spec compatibility\n",
    "\n",
    "* PyMCA was previously often used as a Python library to read Spec files\n",
    "* now prefer using silx\n",
    "\n",
    "```python\n",
    "# instead of\n",
    "from PyMca5.PyMca import specfilewrapper\n",
    "\n",
    "# prefer using\n",
    "from silx.io import specfilewrapper\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## EDF files\n",
    "\n",
    "\n",
    "* ESRF data format\n",
    "* It contains\n",
    "    * 1D/2D/3D array of float/integer\n",
    "    * Header containing various informations\n",
    "    * Multi-frames (more than one image in a single file)\n",
    "    * Often used as file series\n",
    "* Library\n",
    "    * Use FabIO\n",
    "    * silx provides a HDF5-like read access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### reading EDF files using fabIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [2 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "{\n",
      "  \"{\\nHeaderID\": \"EH:000001:000000:000000\",\n",
      "  \"Image\": \"1\",\n",
      "  \"ByteOrder\": \"LowByteFirst\",\n",
      "  \"DataType\": \"UnsignedShort\",\n",
      "  \"Dim_1\": \"256\",\n",
      "  \"Dim_2\": \"256\",\n",
      "  \"Size\": \"131072\",\n",
      "  \"det_orient\": \"1\",\n",
      "  \"d_sample_det\": \"1330\",\n",
      "  \"pixel_zero_y\": \"80\",\n",
      "  \"pixel_zero_x\": \"100\",\n",
      "  \"pixel_size_y\": \"0.055\",\n",
      "  \"pixel_size_x\": \"0.055\",\n",
      "  \"offset\": \"0\",\n",
      "  \"count_time\": \"1\",\n",
      "  \"point_no\": \"101\",\n",
      "  \"scan_no\": \"122\",\n",
      "  \"preset\": \"0\",\n",
      "  \"cnt_col_end\": \"249\",\n",
      "  \"cnt_col_beg\": \"49\",\n",
      "  \"cnt_row_end\": \"236\",\n",
      "  \"cnt_row_beg\": \"36\",\n",
      "  \"col_end\": \"255\",\n",
      "  \"col_beg\": \"0\",\n",
      "  \"row_end\": \"255\",\n",
      "  \"row_beg\": \"0\",\n",
      "  \"sample_pos\": \"4.08 4.08 4.08 90 90 90\",\n",
      "  \"sample_mne\": \"U0 U1 U2 U3 U4 U5\",\n",
      "  \"UB_pos\": \"1.99593e-16 2.73682e-16 -1.54 -1.08894 1.08894 1.6083e-16 1.08894 1.08894 9.28619e-17\",\n",
      "  \"UB_mne\": \"UB0 UB1 UB2 UB3 UB4 UB5 UB6 UB7 UB8\",\n",
      "  \"counter_pos\": \"1 175.682 2365 0 2.6729e-12 22428 0 0 0 -5.53 0 12.3 23.23 26.125 0 -186.8 21.4 22.1 22.2 21.7 1 0 1.37696e+11 5.78566e+06 0 6.91227e-09 0 0 0 0 0 0 0 0 503692 4.26197 -5.60115 -23.1999 1.17698 0.736236 0.86606 503692 414318 0\",\n",
      "  \"counter_mne\": \"sec SRcurr mon det phd2 phd3 attn detc ndetc thcnt mucnt gamcnt delcnt tthcnt dtime Sitemp tfloor tceil tm2b tfe transm phd4 flux meti imcav inbeam outbeam pgo tc1 mfcCO mfcH2 mfcO2 mfcSYN eh1d ccdint delpos thpos gampos Hcnt Kcnt Lcnt ccdintc epoch pgi\",\n",
      "  \"motor_pos\": \"0 167 23.23 12.3 -5.52 90 -13.3 2.8125e-06 0 0.42 0.6176 -0.454 0 0 -2.905 1 1 0 0 0 -3.7 0.5 1.37037 0.25 0 0 0.0015 6 287.188 -1.489 -0.375 10.3608 11.0001 19.5438 18.5666 39.9975 -0.282462 0.235385 0.0162154 8.3 0 0 0 -1 6660 0 0.6 5 0.16 0.24\",\n",
      "  \"motor_mne\": \"dummy mutrans del gam th chi phi bver bhor xt yt zax a2th ath atran delsl gamsl fs3 fs4 mu pssvo pssvg pssho psshg gf_ao1 gf_ao2 ibora cyb ccdy ccdz ccdroll mono nrj u35m u35u u42d t42d t35m t35u pc1 gCO gH2 gO2 gSYN thl thh ssvg sshg pvg phg\",\n",
      "  \"suffix\": \".edf\",\n",
      "  \"prefix\": \"B2_6_\",\n",
      "  \"dir\": \"/data/visitor/ma570/id03/images\",\n",
      "  \"run\": \"925\",\n",
      "  \"title\": \"CCD Image\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import fabio\n",
    "\n",
    "image = fabio.open(\"data/medipix.edf\")\n",
    "\n",
    "# here is the data as a numpy array\n",
    "print(image.data)\n",
    "\n",
    "# here is the header as key-value dictionary\n",
    "print(image.header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### writing files using fabIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import fabio\n",
    "\n",
    "image = numpy.random.rand(10, 10)\n",
    "metadata = {'pixel_size': '0.2'}\n",
    "\n",
    "image = fabio.edfimage.edfimage(data=image, header=metadata)\n",
    "image.write('edf_writing_example.edf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Other formats using FabIO\n",
    "\n",
    "#### Reading other formats\n",
    "\n",
    "FabIO supports image formats from most manufacturers: \n",
    "Mar, Rayonix, Bruker, Dectris, ADSC, Rigaku, Oxford, General Electric…\n",
    "\n",
    "```python\n",
    "import fabio\n",
    "\n",
    "pilatus_image    = fabio.open('filename.cbf')\n",
    "marccd_image     = fabio.open('filename.mccd')\n",
    "\n",
    "tiff_image       = fabio.open('filename.tif')\n",
    "fit2d_mask_image = fabio.open('filename.msk')\n",
    "jpeg_image       = fabio.open('filename.jpg')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### File conversion\n",
    "\n",
    "Using FabIO you can directly convert data to an other format \n",
    "\n",
    "```python\n",
    "import fabio\n",
    "image = fabio.open('data/medipix.edf')\n",
    "image = image.convert('tif')\n",
    "image.save('filename.tif')\n",
    "```\n",
    "(you can also use the command-line fabio-convert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## HDF5 introduction\n",
    "\n",
    "HDF5 (for Hierarchical Data Format) is a file format to structure and store data for high volume and complex data\n",
    "\n",
    "* Hierarchical collection of data (directory and file, UNIX-like path)\n",
    "* High-performance (binary)\n",
    "* Standard exchange format for heterogeneous data\n",
    "* Self-describing extensible types, rich metadata\n",
    "* Support data compression\n",
    "\n",
    "Data can be mostly anything: image, table, graphs, documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### HDF5 description\n",
    "\n",
    "The container is mostly structured with:\n",
    "\n",
    "* **File**: the root of the container\n",
    "* **Group**: a grouping structure containing groups or datasets\n",
    "* **Dataset**: a multidimensional array of data elements\n",
    "* And other features (links, attributes, datatypes)\n",
    "\n",
    "![hdf5_class_diag](images/hdf5_model.png \"hdf5 class diagram\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### HDF5 example\n",
    "\n",
    "Here is an example of the file generated by pyFAI\n",
    "\n",
    "![hdf5_example](images/hdf5_example.png \"hdf5 example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here we read a specific dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First children: ['diff_map_0000', 'diff_map_0001', 'diff_map_0002', 'diff_map_0003', 'diff_map_0004']\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "h5file = h5py.File('data/test.h5')\n",
    "\n",
    "# print available names at the first level\n",
    "print(\"First children:\", list(h5file['/'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: (29, 78, 100) 226200 float32\n"
     ]
    }
   ],
   "source": [
    "# reaching a dataset from a sub group\n",
    "dataset = h5file['/diff_map_0004/data/map']\n",
    "\n",
    "# using size and types to not read the full stored data\n",
    "print(\"Dataset:\", dataset.shape, dataset.size, dataset.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "datasets mimics numpy-array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104.14766  103.352615 103.01642  103.24001  103.27751 ]\n",
      "[205.95827 206.2795  206.5441  206.48112 206.46625]\n"
     ]
    }
   ],
   "source": [
    "# read and apply the operation\n",
    "print(dataset[5, 5, 0:5])\n",
    "print(2 * dataset[0, 5, 0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[103.45841  103.19393  103.12445  103.15601  103.203285]\n",
      "[0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# copy the data and store it as a numpy-array\n",
    "b = dataset[...]\n",
    "b[0, 0, 0:5] = 0\n",
    "print(dataset[0, 0, 0:5])\n",
    "print(b[0, 0, 0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### h5py write example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import h5py\n",
    "\n",
    "data = numpy.arange(10000.0)\n",
    "data.shape = 100, 100\n",
    "\n",
    "# write\n",
    "h5file = h5py.File('my_first_one.h5', mode='w')\n",
    "\n",
    "# write data into a dataset from the root\n",
    "h5file['/data1'] = data\n",
    "\n",
    "# write data into a dataset from group1\n",
    "h5file['/group1/data2'] = data\n",
    "\n",
    "h5file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Usefull tools for HDF5\n",
    "\n",
    "* h5ls, h5dump, hdfview\n",
    "```bash\n",
    ">>> h5ls -r my_first_one.h5 \n",
    ">>> /                        Group\n",
    ">>> /data1                   Dataset {100, 100}\n",
    ">>> /group1                  Group\n",
    ">>> /group1/data2            Dataset {100, 100}\n",
    "```\n",
    "\n",
    "* h5py\n",
    "* silx\n",
    "* silx view\n",
    "\n",
    "==> The HDF group provides a web page with more tools https://support.hdfgroup.org/HDF5/doc/RM/Tools.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## silx io\n",
    "\n",
    "* Try to simplify the transition to HDF5\n",
    "    * Provide a h5py-like API on top of format used at ESRF\n",
    "    * Single way to access to Spec/EDF/HDF5 files\n",
    "    * Based on NeXus specifications http://www.nexusformat.org/\n",
    "* Read-only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Read HDF5 using silx\n",
    "\n",
    "For conveniance, ``silx`` also provides the h5py API for HDF5 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import silx.io\n",
    "\n",
    "h5file = silx.io.open('data/test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First children: ['diff_map_0000', 'diff_map_0001', 'diff_map_0002', 'diff_map_0003', 'diff_map_0004']\n"
     ]
    }
   ],
   "source": [
    "# print available names at the first level\n",
    "print(\"First children:\", list(h5file['/'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: (29, 78, 100) 226200 float32\n"
     ]
    }
   ],
   "source": [
    "# reaching a dataset from a sub group\n",
    "dataset = h5file['/diff_map_0004/data/map']\n",
    "\n",
    "# using size and types to not read the full stored data\n",
    "print(\"Dataset:\", dataset.shape, dataset.size, dataset.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### spec files using silx\n",
    "\n",
    "Silx can also expose spec file with a HDF5-like mapping\n",
    "\n",
    "#### HDF5-like mapping  (given for general information)\n",
    "\n",
    "![mapping_spec](images/spech5_arrows.png \"hdf5-like mapping for spec files\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reading spec files using silx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First childs: odict_keys(['94.1', '95.1', '96.1'])\n",
      "Containt of measurement: odict_keys(['delta', 'H', 'K', 'L', 'Epoch', 'Seconds', 'Detector', 'Ion_m1', 'Ion_m2', 'srcur', 'curratt', 'ratio', 'all1', 'psd1', 'dir1', 'refl1', 'yoneda1', 'ACEdet', 'mcaLt1', 'twago', 'bpmi', 'tlangm', 'vO2', 'apdcnt', 'apdtemp', 'Monitor', 'detcorr', 'mca_0'])\n",
      "(2011829.0, 6.247502e-07)\n",
      "(2011831.0, 6.253457e-07)\n",
      "(2011833.0, 6.258715e-07)\n",
      "(2011836.0, 6.258831e-07)\n",
      "(2011838.0, 6.255509e-07)\n",
      "(2011840.0, 6.253553e-07)\n",
      "(2011842.0, 6.257577e-07)\n",
      "(2011844.0, 6.25633e-07)\n",
      "(2011846.0, 6.25749e-07)\n",
      "(2011848.0, 6.257645e-07)\n",
      "(2011851.0, 6.256978e-07)\n",
      "(2011853.0, 6.259445e-07)\n",
      "(2011855.0, 6.258398e-07)\n",
      "(2011857.0, 6.257358e-07)\n",
      "(2011859.0, 6.258171e-07)\n",
      "(2011861.0, 6.257585e-07)\n",
      "(2011863.0, 6.258035e-07)\n",
      "(2011866.0, 6.25884e-07)\n",
      "(2011868.0, 6.256461e-07)\n",
      "(2011870.0, 6.258715e-07)\n",
      "(2011872.0, 6.257519e-07)\n"
     ]
    }
   ],
   "source": [
    "import silx.io\n",
    "data = silx.io.open('data/oleg.dat')\n",
    "\n",
    "# print available scans\n",
    "print(\"First childs:\", data['/'].keys())\n",
    "\n",
    "# print available measurements from the scan 94.1\n",
    "print(\"Containt of measurement:\", data['/94.1/measurement'].keys())\n",
    "\n",
    "# get data from measurement\n",
    "xdata = data['/94.1/measurement/Epoch']\n",
    "ydata = data['/94.1/measurement/bpmi']\n",
    "for row in zip(xdata, ydata):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For more information and examples you can read the silx IO tutorial: https://github.com/silx-kit/silx-training/blob/master/silx/io/io.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### EDF files using silx\n",
    "\n",
    "Silx can also expose spec file with a HDF5-like mapping\n",
    "\n",
    "#### HDF5-like mapping (given for general information)\n",
    "\n",
    "![mapping_spec](images/fabioh5_arrows.png \"hdf5-like mapping for EDF files\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Read EDF file using silx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frames: 6\n",
      "Size of an image: (540, 640)\n",
      "(1465802989.9281, 0.0, 0)\n",
      "(1465803040.360028, 14.371199607849121, 0)\n",
      "(1465803090.780985, 28.742399215698242, 0)\n",
      "(1465803140.696993, 42.969888, 0)\n",
      "(1465803191.6209, 57.484798431396484, 0)\n",
      "(1465803266.291432, 71.85600280761719, 0)\n"
     ]
    }
   ],
   "source": [
    "import silx.io\n",
    "data = silx.io.open('data/ID16B_diatomee.edf')\n",
    "\n",
    "# Access to the frames\n",
    "frames = data['/scan_0/instrument/detector_0/data']\n",
    "len(frames)  # number of frames\n",
    "frames[0]    # first frame\n",
    "print(\"Number of frames:\", len(frames))\n",
    "print(\"Size of an image:\", frames[0].shape)\n",
    "\n",
    "# Access to motors, monitor, timestanp\n",
    "srot = data['scan_0/instrument/positioners/srot'][...]\n",
    "mon = data['scan_0/measurement/mon'][...]\n",
    "timestamp = data['scan_0/instrument/detector_0/others/time_of_day'][...]\n",
    "for row in zip(timestamp, srot, mon):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Silx Tools / utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### silx.io.utils.h5ls\n",
    "List tree contains\n",
    "`h5ls` allow you to display the tree contained into an HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+diff_map_0000\n",
      "\t+data\n",
      "\t\t<HDF5 dataset \"map\": shape (29, 78, 100), type \"<f4\">\n",
      "\t<HDF5 dataset \"program_name\": shape (), type \"|S5\">\n",
      "\t+pyFAI\n",
      "\t\t<HDF5 dataset \"PONIfile\": shape (), type \"|S4\">\n",
      "\t\t<HDF5 dataset \"date\": shape (), type \"|S25\">\n",
      "\t\t<HDF5 dataset \"detector\": shape (), type \"|S9\">\n",
      "\t\t<HDF5 dataset \"dim0\": shape (), type \"<i8\">\n",
      "\t\t<HDF5 dataset \"dim1\": shape (), type \"<i8\">\n",
      "\t\t<HDF5 dataset \"dim2\": shape (), type \"<i8\">\n",
      "\t\t<HDF5 dataset \"dist\": shape (), type \"<f8\">\n",
      "\t\t<HDF5 dataset \"inputfiles\": shape (2230,), type \"|S80\">\n",
      "\t\t<HDF5 dataset \"pixel1\": shape (), type \"<f8\">\n",
      "\t\t<HDF5 dataset \"pixel2\": shape (), type \"<f8\">\n",
      "\t\t<HDF5 dataset \"poni1\": shape (), type \"<f8\">\n",
      "\t\t<HDF5 dataset \"poni2\": shape (), type \"<f8\">\n",
      "\t\t<HDF5 dataset \"program\": shape (1,), type \"|S8\">\n",
      "\t\t<HDF5 dataset \"rot1\": shape (), type \"<f8\">\n",
      "\t\t<HDF5 dataset \"rot2\": shape (), type \"<f8\">\n",
      "\t\t<HDF5 dataset \"rot3\": shape (), type \"<f8\">\n",
      "\t\t<HDF5 dataset \"version\": shape (), type \"|S6\">\n",
      "\t\t<HDF5 dataset \"wavelength\": shape (), type \"<f8\">\n",
      "\t<HDF5 dataset \"start_time\": shape (), type \"|S25\">\n",
      "\t<HDF5 dataset \"title\": shape (), type \"|S7\">\n",
      "+diff_map_0001\n",
      "\t+data\n",
      "\t\t<HDF5 dataset \"map\": shape (29, 78, 100), type \"<f4\">\n",
      "\t<HDF5 dataset \"program_name\": shape (), type \"|S5\">\n",
      "\t+pyFAI\n",
      "\t\t<HDF5 dataset \"PONIfile\": shape (), type \"|S4\">\n",
      "\t\t<HDF5 dataset \"date\": shape (), type \"|S25\">\n",
      "\t\t<HDF5 dataset \"detector\": shape (), type \"|S9\">\n",
      "\t\t<HDF5 dataset \"dim0\": shape (), type \"<i8\">\n",
      "\t\t<HDF5 dataset \"dim1\": shape (), type \"<i8\">\n",
      "\t\t<HDF5 dataset \"dim2\": shape (), type \"<i8\">\n",
      "\t\t<HDF5 dataset \"dist\": shape (), type \"<f8\">\n",
      "\t\t<HDF5 dataset \"inputfiles\": shape (2230,), type \"|S80\">\n",
      "\t\t<HDF5 dataset \"pixel1\": shape (), type \"<f8\">\n",
      "\t\t<HDF5 dataset \"pixel2\": shape (), type \"<f8\">\n",
      "\t\t<HDF5 dataset \"poni1\": shape (), type \"<f8\">\n",
      "\t\t<HDF5 dataset \"poni2\": shape (), type \"<f8\">\n",
      "\t\t<HDF5 dataset \"program\": shape (1,), type \"|S8\">\n",
      "\t\t<HDF5 dataset \"rot1\": shape (), type \"<f8\">\n",
      "\t\t<HDF5 dataset \"rot2\": shape (), type \"<f8\">\n",
      "\t\t<HDF5 dataset \"rot3\": shape (), type \"<f8\">\n",
      "\t\t<HDF5 dataset \"version\": shape (), type \"|S6\">\n",
      "\t\t<HDF5 dataset \"wavelength\": shape (), type \"<f8\">\n",
      "\t<HDF5 dataset \"start_time\": shape (), type \"|S25\">\n",
      "\t<HDF5 dataset \"title\": shape (), type \"|S7\">\n",
      "+diff_map_0002\n",
      "\t+data\n",
      "\t\t<HDF5 dataset \"map\": shape (29, 78, 100), type \"<f4\">\n",
      "\t<HDF5 dataset \"program_name\": shape (), type \"|S5\">\n",
      "\t+pyFAI\n",
      "\t\t<HDF5 dataset \"PONIfile\": shape (), type \"|S4\">\n",
      "\t\t<HDF5 dataset \"date\": shape (), type \"|S25\">\n",
      "\t\t<HDF5 dataset \"detector\": shape (), type \"|S9\">\n",
      "\t\t<HDF5 dataset \"dim0\": shape (), type \"<i8\">\n",
      "\t\t<HDF5 dataset \"dim1\": shape (), type \"<i8\">\n",
      "\t\t<HDF5 dataset \"dim2\": shape (), type \"<i8\">\n",
      "\t\t<HDF5 dataset \"dist\": shape (), type \"<f8\">\n",
      "\t\t<HDF5 dataset \"inputfiles\": shape (2230,), type \"|S80\">\n",
      "\t\t<HDF5 dataset \"pixel1\": shape (), type \"<f8\">\n",
      "\t\t<HDF5 dataset \"pixel2\": shape (), type \"<f8\">\n",
      "\t\t<HDF5 dataset \"poni1\": shape (), type \"<f8\">\n",
      "\t\t<HDF5 dataset \"poni2\": shape (), type \"<f8\">\n",
      "\t\t<HDF5 dataset \"program\": shape (1,), type \"|S8\">\n",
      "\t\t<HDF5 dataset \"rot1\": shape (), type \"<f8\">\n",
      "\t\t<HDF5 dataset \"rot2\": shape (), type \"<f8\">\n",
      "\t\t<HDF5 dataset \"rot3\": shape (), type \"<f8\">\n",
      "\t\t<HDF5 dataset \"version\": shape (), type \"|S6\">\n",
      "\t\t<HDF5 dataset \"wavelength\": shape (), type \"<f8\">\n",
      "\t<HDF5 dataset \"start_time\": shape (), type \"|S25\">\n",
      "\t<HDF5 dataset \"title\": shape (), type \"|S7\">\n",
      "+diff_map_0003\n",
      "\t+data\n",
      "\t\t<HDF5 dataset \"2th\": shape (100,), type \"<f8\">\n",
      "\t\t<HDF5 dataset \"map\": shape (29, 78, 100), type \"<f4\">\n",
      "\t<HDF5 dataset \"program_name\": shape (), type \"|S5\">\n",
      "\t+pyFAI\n",
      "\t\t<HDF5 dataset \"PONIfile\": shape (), type \"|S4\">\n",
      "\t\t<HDF5 dataset \"date\": shape (), type \"|S25\">\n",
      "\t\t<HDF5 dataset \"detector\": shape (), type \"|S9\">\n",
      "\t\t<HDF5 dataset \"dim0\": shape (), type \"<i8\">\n",
      "\t\t<HDF5 dataset \"dim1\": shape (), type \"<i8\">\n",
      "\t\t<HDF5 dataset \"dim2\": shape (), type \"<i8\">\n",
      "\t\t<HDF5 dataset \"dist\": shape (), type \"<f8\">\n",
      "\t\t<HDF5 dataset \"inputfiles\": shape (2230,), type \"|S80\">\n",
      "\t\t<HDF5 dataset \"pixel1\": shape (), type \"<f8\">\n",
      "\t\t<HDF5 dataset \"pixel2\": shape (), type \"<f8\">\n",
      "\t\t<HDF5 dataset \"poni1\": shape (), type \"<f8\">\n",
      "\t\t<HDF5 dataset \"poni2\": shape (), type \"<f8\">\n",
      "\t\t<HDF5 dataset \"program\": shape (1,), type \"|S8\">\n",
      "\t\t<HDF5 dataset \"rot1\": shape (), type \"<f8\">\n",
      "\t\t<HDF5 dataset \"rot2\": shape (), type \"<f8\">\n",
      "\t\t<HDF5 dataset \"rot3\": shape (), type \"<f8\">\n",
      "\t\t<HDF5 dataset \"version\": shape (), type \"|S6\">\n",
      "\t\t<HDF5 dataset \"wavelength\": shape (), type \"<f8\">\n",
      "\t<HDF5 dataset \"start_time\": shape (), type \"|S25\">\n",
      "\t<HDF5 dataset \"title\": shape (), type \"|S7\">\n",
      "+diff_map_0004\n",
      "\t+data\n",
      "\t\t<HDF5 dataset \"2th\": shape (100,), type \"<f8\">\n",
      "\t\t<HDF5 dataset \"map\": shape (29, 78, 100), type \"<f4\">\n",
      "\t<HDF5 dataset \"end_time\": shape (), type \"|O\">\n",
      "\t<HDF5 dataset \"program_name\": shape (), type \"|S5\">\n",
      "\t+pyFAI\n",
      "\t\t<HDF5 dataset \"PONIfile\": shape (), type \"|S4\">\n",
      "\t\t<HDF5 dataset \"date\": shape (), type \"|S25\">\n",
      "\t\t<HDF5 dataset \"detector\": shape (), type \"|S8\">\n",
      "\t\t<HDF5 dataset \"dim0\": shape (), type \"<i8\">\n",
      "\t\t<HDF5 dataset \"dim1\": shape (), type \"<i8\">\n",
      "\t\t<HDF5 dataset \"dim2\": shape (), type \"<i8\">\n",
      "\t\t<HDF5 dataset \"dist\": shape (), type \"<f8\">\n",
      "\t\t<HDF5 dataset \"inputfiles\": shape (2230,), type \"|S80\">\n",
      "\t\t<HDF5 dataset \"pixel1\": shape (), type \"<f8\">\n",
      "\t\t<HDF5 dataset \"pixel2\": shape (), type \"<f8\">\n",
      "\t\t<HDF5 dataset \"poni1\": shape (), type \"<f8\">\n",
      "\t\t<HDF5 dataset \"poni2\": shape (), type \"<f8\">\n",
      "\t\t<HDF5 dataset \"program\": shape (1,), type \"|S8\">\n",
      "\t\t<HDF5 dataset \"rot1\": shape (), type \"<f8\">\n",
      "\t\t<HDF5 dataset \"rot2\": shape (), type \"<f8\">\n",
      "\t\t<HDF5 dataset \"rot3\": shape (), type \"<f8\">\n",
      "\t\t<HDF5 dataset \"splineFile\": shape (), type \"|S71\">\n",
      "\t\t<HDF5 dataset \"version\": shape (), type \"|S6\">\n",
      "\t\t<HDF5 dataset \"wavelength\": shape (), type \"<f8\">\n",
      "\t<HDF5 dataset \"start_time\": shape (), type \"|S25\">\n",
      "\t<HDF5 dataset \"title\": shape (), type \"|S7\">\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import silx.io\n",
    "import silx.io.utils\n",
    "\n",
    "h5file = silx.io.open('data/test.h5')\n",
    "\n",
    "string = silx.io.utils.h5ls(h5file)\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### silx.io.convert.write_to_h5\n",
    "\n",
    "Convert spec file to HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from silx.io.convert import write_to_h5\n",
    "\n",
    "write_to_h5('data/oleg.dat', 'oleg.h5', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 payno soft 688824 Sep 20 16:35 oleg.h5\r\n"
     ]
    }
   ],
   "source": [
    "ls -al oleg.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Exercise\n",
    "========\n",
    "\n",
    "1. Read the EDF file ``medipix.edf``.\n",
    "2. Process the data\n",
    "   The goal of the processing is to clamp the pixels values to a new range of values ([10%, 90%] of the existing one). To do so:\n",
    "\n",
    "   - Create a mask to detect pixel which are below 10% or above 90% of the current range.\n",
    "   - With the above mask, set the affected pixels to 10% 'low value'.\n",
    "\n",
    "3. Store the source, the mask of changed pixels and the result inside ``process.h5``, as below.\n",
    "\n",
    "   ![Output file structure](images/exercise-result.png)\n",
    "\n",
    "4. Load ``process.h5`` and list the root content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Load data/medipix.edf\n",
    "# ...\n",
    "\n",
    "# Process the data\n",
    "# ...\n",
    "\n",
    "# Save data into a new file (process.h5)\n",
    "# ...\n",
    "\n",
    "# Load process.h5 and list the root content\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Solution\n",
    "========"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Load data/medipix.edf\n",
    "import exercicesolution\n",
    "import inspect\n",
    "print(inspect.getsource(exercicesolution.load_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# process data\n",
    "import exercicesolution\n",
    "import inspect\n",
    "print(inspect.getsource(exercicesolution.process_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# save data\n",
    "import exercicesolution\n",
    "import inspect\n",
    "print(inspect.getsource(exercicesolution.save_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# list root\n",
    "import exercicesolution\n",
    "import inspect\n",
    "print(inspect.getsource(exercicesolution.list_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root level:\n",
      "['mask', 'raw', 'result']\n"
     ]
    }
   ],
   "source": [
    "# result\n",
    "import exercicesolution\n",
    "raw_data, proc_data, mask = exercicesolution.solution(\"data/medipix.edf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0c14ae1710>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imshow(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0c10a43eb8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imshow(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0c10a029b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imshow(proc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "Preconized library according to the use case and the file format.\n",
    "\n",
    "| Formats              | Read       | Write |\n",
    "|----------------------|------------|-------|\n",
    "| HDF5                 | silx/h5py  | h5py  |\n",
    "| Specfile             | silx       |       |\n",
    "| EDF multiframe       | silx/fabio | fabio |\n",
    "| EDF                  | fabio      | fabio |\n",
    "| Other raster formats | fabio      | fabio |"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
